{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5512190-6089-45f6-999f-0f3cd0c07548",
   "metadata": {},
   "source": [
    "# Accelerating Transformers with Hugging Face Optimum\n",
    "\n",
    "Lewis Tunstall (open-source @ Hugging Face)\n",
    "\n",
    "`@_lewtun`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29d3e0-a464-4082-bff0-0182db3f32c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf20df-151e-4260-ba12-c6bfe90eec3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7dbd23-050e-4aa3-ba12-da722f6fa96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e53d3-b56a-43a7-9807-48d01ea2c95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e6104-e455-4b06-85b7-96782c0d6291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fac18b-d473-46fa-af3e-4cb632f771a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b808e27-e3b0-4a36-bd44-cdaa295e2d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1374980-6c13-4a6e-aa29-e23752d0da5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a357469-00a8-43cc-8bfe-f491e367b3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98beeef6-8c32-4588-b298-1298995427da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "def9147f-41c2-4411-b8d7-5039f0df1d58",
   "metadata": {},
   "source": [
    "## Who is Lewis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5ce5e-c30c-47bf-ae76-0af809c1f078",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/about.png\" alt=\"About me\" title=\"Title text\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f44e9c-4aaf-4f34-a165-69c85c9a49ce",
   "metadata": {},
   "source": [
    "* PhD in Physics from University of Adelaide, Australia\n",
    "* Co-author of O'Reilly book [_Natural Language Processing with Transformers_](https://transformersbook.com/)\n",
    "* Co-developer of the **free** [Hugging Face course](https://huggingface.co/course/chapter1/1)\n",
    "* Maintainer of ONNX API in `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fd108-ca54-409c-908c-2efbc541e9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c7f1b5f-f5ba-4571-a794-2fbf697e595b",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* What is Optimum?\n",
    "* Question answering as a case study\n",
    "* Making models faster with quantization\n",
    "* Optimizing inference with ONNX and ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ca919-fb1d-45f7-8871-b9ca78c13e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "860983ce-0a16-45bf-aa96-fc39c6498e2a",
   "metadata": {},
   "source": [
    "## What is Optimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e793d56-9a67-41a9-9236-78b5466d6acf",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/optimum.jpeg\" alt=\"About me\" title=\"Title text\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222d57d-14a1-4893-94e4-71eaaf829196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc859a79-b61b-43a3-85cd-c90b5300e365",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/hardware.png\" alt=\"About me\" title=\"Title text\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f55c1-13c1-49e1-a592-821f7d16a932",
   "metadata": {},
   "source": [
    "Today:\n",
    "\n",
    "* Running inference with **ONNX Runtime** in Optimum\n",
    "* Dynamic quantization as a demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4d0ef-a6a6-47bc-bd20-84d67b2b159f",
   "metadata": {},
   "source": [
    "## Question answering as a case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0374c68-4569-455e-acd9-be3681e6e916",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/marie-curie.png\" alt=\"About me\" title=\"Title text\" width=800>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d388663-dbdb-4687-ba0f-db839e5a86c8",
   "metadata": {},
   "source": [
    "* Low latencies critical for user experience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b1b46-4ea0-457e-b5f8-fca0b7ac7e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d0db8f6-c8c0-47ea-bf2f-4ceebef4515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"https://hf.space/gradioiframe/abidlabs/question-answering-simple/+\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0e85c2b640>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.display.IFrame(\"https://hf.space/gradioiframe/abidlabs/question-answering-simple/+\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "192d9d8a-e43f-43d1-bb31-c1265f82941a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7219798564910889, 'start': 277, 'end': 281, 'answer': '1903'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "context = \"\"\"Marie Sklodowska was born in Warsaw, Poland, to a family of teachers who believed strongly in education. She moved to Paris to continue her studies and there met Pierre Curie, who became both her husband and colleague in the field of radioactivity. The couple later shared the 1903 Nobel Prize in Physics. Marie was widowed in 1906, but continued the couple's work and went on to become the first person ever to be awarded two Nobel Prizes. During World War I, Curie organized mobile X-ray teams. The Curies' daughter, Irene, was also jointly awarded the Nobel Prize in Chemistry alongside her husband, Frederic Joliot.\"\"\"\n",
    "question = \"when did marie curie win her first nobel prize?\"\n",
    "pred = question_answerer(question, context)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921fc405-34cc-4760-8158-9d6017a9664c",
   "metadata": {},
   "source": [
    "* Model looks good, deploy to prod?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d3ac2-f8c6-4696-b877-578b33dc7468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df542d6b-02b7-4339-bed5-cf3b24570c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b8491-1490-4785-ab43-bce0f3fcadac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53eafe5a-7654-423a-bf6a-e4d475434125",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/prod.jpeg\" alt=\"About me\" title=\"Title text\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ec027-081d-4cde-a9ad-ad1e639ef11d",
   "metadata": {},
   "source": [
    "Deployment involves tradoff among several constraints:\n",
    "\n",
    "* Model performance (accuracy, F1 score etc)\n",
    "* Latency\n",
    "* Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f81d9c14-1f15-47e1-9681-1745b5991ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(question=question, context=context)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(question=question, context=context)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    return f\"average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e39e6f3-d5ea-4a14-ae34-937ab5948ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model average latency (ms) - 97.11 +\\- 0.17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vanilla model {measure_latency(question_answerer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233f44f-b3ce-4483-8976-49d537c1e549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f907e9-8392-4295-bf77-0f4c88eacb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f89e7bc8-2e93-4765-8bb1-330691b4354a",
   "metadata": {},
   "source": [
    "## Making models faster with quantization\n",
    "\n",
    "Basic idea:\n",
    "\n",
    "* Represent weights and activations with **low-precision data types** like 8-bit integer instead of 32-bit floating point.\n",
    "* Less memory storage & faster matmuls!\n",
    "\n",
    "In practice:\n",
    "\n",
    "* Map range $[f_\\mathrm{min}, f_\\mathrm{max}]$ of FP values to smaller range $[q_\\mathrm{min}, q_\\mathrm{max}]$:\n",
    "\n",
    "$$ f = \\left(\\frac{f_\\mathrm{max} - f_\\mathrm{min}}{q_\\mathrm{max} - q_\\mathrm{min}} \\right)(q - Z) = S(q-Z) $$\n",
    "\n",
    "* $S$ is _scale factor_ and $Z$ the _zero point_ (where the quantized value of $f=0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb9a36-1efd-4b14-8ed5-cdd59c552f22",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/fp32-to-int8.png\" alt=\"Quantization\" title=\"Title text\">\n",
    "</div>\n",
    "\n",
    "_Figure courtesy of Manas Sahni_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8776304-7ed8-441c-943c-4f827f8618c7",
   "metadata": {},
   "source": [
    "Three main ways to quantize:\n",
    "\n",
    "* **Dynamic quantization:** quantize weights & activations on-the-fly. Simplest to start with.\n",
    "* **Static quantization:** precompute quantization scheme by observing activation patterns on sample of data. Generally ives better latency, but more complex to calibrate\n",
    "* **Quantization aware training:** simulate quantization during training with \"fake\" quantization of FP32 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f0449-984e-4af3-be2f-c71f1991d9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "500a472a-f6ce-48d4-93e2-c4932a50835b",
   "metadata": {},
   "source": [
    "## Optimizing inference with ONNX and ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f5a2b-4a7a-4889-90b9-ca125c1109b0",
   "metadata": {},
   "source": [
    "### Step 1: Install Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe36b01-8128-4438-9fb9-0c6e5365f85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84785ee0-451a-45c9-bd51-ddcec03041c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"optimum[onnxruntime]==1.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984eaca-027c-41ed-b042-355663eeb847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a17b5a36-8059-45b8-96ad-04f301dce78e",
   "metadata": {},
   "source": [
    "### Step 2: Export the model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd5685f1-c917-4b4c-bcd8-598f152a5e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23207e424004f97b150c10d85559230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('onnx/tokenizer_config.json',\n",
       " 'onnx/special_tokens_map.json',\n",
       " 'onnx/vocab.json',\n",
       " 'onnx/merges.txt',\n",
       " 'onnx/added_tokens.json',\n",
       " 'onnx/tokenizer.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task = \"question-answering\"\n",
    "\n",
    "# Load PyTorch weights and convert to ONNX\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Save ONNX checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9016a2a3-5abe-4e17-be3a-3b36df9ea81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  model-optimized.onnx     tokenizer_config.json\n",
      "merges.txt   model-quantized.onnx     tokenizer.json\n",
      "model.onnx   special_tokens_map.json  vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls \"onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541a9e1-ea46-43fc-b61c-eb7b35d47601",
   "metadata": {},
   "source": [
    "### Step 3: Quantize the model with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e922c46d-5692-45e6-9973-eda27882c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(model_id, feature=task)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "quantized_path = quantizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_quantized_model_output_path=onnx_path / \"model-quantized.onnx\",\n",
    "    quantization_config=qconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d630192f-65eb-4e24-8a12-83dcd59dfe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Onnx Model file size: 473.34 MB\n",
      "Quantized Onnx Model file size: 230.83 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Vanilla Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model-quantized.onnx\")/(1024*1024)\n",
    "print(f\"Quantized Onnx Model file size: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711966fd-9dcc-4b8d-b525-73689a420198",
   "metadata": {},
   "source": [
    "### Step 4: Run inference with Transformers pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b8de1-a513-45e8-b17d-1cb50fd96889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ec81e44-7784-4d5f-bc77-68dbb1f33d08",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/cat.png\" alt=\"Quantization\" title=\"Title text\", width=400>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae914d2-fd44-41ba-9f5b-df0417926eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298af6c-0476-4f94-9ce6-d10cd24ad1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad9cd9-9a4d-4ec1-8eb4-3fc373b7632b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3a95b-93e1-45f6-bc3d-ae8cff75ff06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a13891c-e0fc-498e-b5f5-ca49e0a108bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4670557975769043, 'start': 277, 'end': 281, 'answer': '1903'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load quantized model\n",
    "quantized_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model-quantized.onnx\")\n",
    "\n",
    "# test the quantized model with using transformers pipeline\n",
    "quantized_optimum_qa = pipeline(task, model=quantized_model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = quantized_optimum_qa(question=question, context=context)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15bb048c-570f-4c87-a1ba-3ea5a7a5734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model average latency (ms) - 41.55 +\\- 7.63\n"
     ]
    }
   ],
   "source": [
    "print(f\"Quantized model {measure_latency(quantized_optimum_qa)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4467fec-4634-4d46-a2a5-be74dd0df445",
   "metadata": {},
   "source": [
    "Nice, dynamic quantization gave a 2x speed-up ðŸ¤¯!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb508d-0260-436f-b76f-0326bfd49d36",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3dea108a-a3a7-4183-9a42-a3ac8d497359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73432274f95242fd870fed74922e789d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset 11873\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.logging.set_verbosity_error()\n",
    "from datasets import load_metric,load_dataset\n",
    "\n",
    "metric = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"squad_v2\")[\"validation\"]\n",
    "\n",
    "print(f\"length of dataset {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd06b1-c4d4-4cd6-aa71-b649890b4a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(example):\n",
    "    default = optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "    quantized = quantized_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "    return {\n",
    "      'reference': {'id': example['id'], 'answers': example['answers']},\n",
    "      'default': {'id': example['id'],'prediction_text': default['answer'], 'no_answer_probability': 0.},\n",
    "      'quantized': {'id': example['id'],'prediction_text': quantized['answer'], 'no_answer_probability': 0.},\n",
    "      }\n",
    "\n",
    "result = dataset.shuffle(seed=42).select(range(2000)).map(evaluate, num_proc=4)\n",
    "# COMMENT IN to run evaluation on whoel validation set - takes ~1.5h\n",
    "# result = dataset.map(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a0a031d4-7428-4270-82bc-316e568ee7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_metrics = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\n",
    "quantized_metrics = metric.compute(predictions=result[\"quantized\"], references=result[\"reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5573b93a-713d-4269-8ec8-f95e7e20a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model: exact=79.1% f1=81.99621377859617%\n",
      "quantized model: exact=78.55% f1=81.33489049340754%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vanilla model: Exact match={default_acc['exact']}% F1={default_acc['f1']}%\")\n",
    "print(f\"Quantized model: Exact match={quantized['exact']}% F1={quantized['f1']}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
