{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Quantization example with `optimum` for `distilbert`\n",
    "\n",
    "In this session, you will learn how to do post-training static quantization on Hugging Face Transformers model. The session will show you how to quantize a DistilBERT model using [Hugging Face Optimum](https://huggingface.co/docs/optimum/index) and [ONNX Runtime](https://onnxruntime.ai/). Hugging Face Optimum is an extension of ðŸ¤— Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware.\n",
    "\n",
    "Note: Static quantization is currently only supported for CPUs, so we will not be utilizing GPUs / CUDA in this session. By the end of this session, you see how quantization with Hugging Face Optimum can result in significant increase in model latency while keeping almost 100% of the full-precision model. Furthermore, youâ€™ll see how to easily apply some advanced quantization and optimization techniques shown here so that your models take much less of an accuracy hit than they would otherwise. \n",
    "\n",
    "You will learn how to:\n",
    "1. Setup Development Environment\n",
    "2. Convert a Hugging Face `Transformers` model to ONNX for inference\n",
    "3. Configure static quantization & run Calibration of quantization ranges\n",
    "4. Use the ORTQuantizer to apply static quantization\n",
    "5. Test inference with the quantized model\n",
    "6. Evaluate the performance and speed\n",
    "7. Push the quantized model to the Hub\n",
    "8. Load and run inference with a quantized model from the hub\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "\n",
    "_This tutorial was created and run on an c6i.xlarge AWS EC2 Instance._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Optimum with the onnxruntime utilities and evaluate.\n",
    "\n",
    "This will install all required packages for us including transformers, torch, and onnxruntime. If you are going to use a GPU you can install optimum with pip install optimum[onnxruntime-gpu]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the mkl-include and mkl dependencies if running on Colab\n",
    "%pip install \"optimum[onnxruntime]==1.2.2\" evaluate[evaluator] sklearn mkl-include mkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert a Hugging Face `Transformers` model to ONNX for inference\n",
    "\n",
    "Before we can start qunatizing we need to convert our vanilla `transformers` model to the `onnx` format. To do this we will use the new [ORTModelForSequenceClassification](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling_ort#optimum.onnxruntime.ORTModelForSequenceClassification) class calling the `from_pretrained()` method with the `from_transformers` attribute. The model we are using is the [optimum/distilbert-base-uncased-finetuned-banking77](https://huggingface.co/optimum/distilbert-base-uncased-finetuned-banking77) a fine-tuned DistilBERT model on the Banking77 dataset achieving an Accuracy score of `92.5` and as the feature (task) `text-classification`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_id=\"optimum/distilbert-base-uncased-finetuned-banking77\"\n",
    "dataset_id=\"banking77\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure static quantization & run Calibration of quantization ranges\n",
    "\n",
    "Post-training static quantization compared to dynamic quantization not only involves converting the weights from float to int, but also performing an first additional step of feeding the data through the model to compute the distributions of the different activations (calibration ranges). These distributions are then used to determine how the different activations should be quantized at inference time. \n",
    "Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.\n",
    "\n",
    "First step is to create our Quantization configuration using `optimum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from onnxruntime.quantization import QuantFormat, QuantizationMode\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(model_id, feature=model.pipeline_task)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(\n",
    "    is_static=True,\n",
    "    format=QuantFormat.QOperator,\n",
    "    mode=QuantizationMode.QLinearOps,\n",
    "    per_channel=True,\n",
    "    operators_to_quantize=[\"MatMul\", \"Add\" ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have configured our configuration we are going to use the fine-tuning dataset as calibration data to calculate the quantization parameters of activations. The `ORTQuantizer` supports three calibration methods: MinMax, Entropy and Percentile.\n",
    "\n",
    "We are going to use Percentile as calibration method. For the session we have already run hyperparameter optimization in advance to find the right `perecentiles` to achieve the highest accuracy. There for we used the `scripts/run_static_quantizatio_hpo.py` together with `optuna`.\n",
    "\n",
    "Finding the right calibration method and percentiles is what make static quantization cost-intensive. Since it can take up to multiple hours to find the right values and there is sadly no rule of thumb. \n",
    "If you want to learn more about it you should check out the \"[INTEGER QUANTIZATION FOR DEEP LEARNING INFERENCE:\n",
    "PRINCIPLES AND EMPIRICAL EVALUATION](https://arxiv.org/pdf/2004.09602.pdf)\" paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b/cache-879d5f95a9fbe7e7.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b/cache-a6a114c6cdb61d64.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Finding optimal threshold for each tensor using percentile algorithm ...\n",
      "Number of tensors : 218\n",
      "Number of histogram bins : 2048\n",
      "Percentile : (0.00760919092822121,99.99239080907178)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from optimum.onnxruntime.configuration import AutoCalibrationConfig\n",
    "\n",
    "def preprocess_fn(ex, tokenizer):\n",
    "    return tokenizer(ex[\"text\"],padding=\"longest\")\n",
    "\n",
    "# Create the calibration dataset\n",
    "calibration_samples = 256\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    dataset_id,\n",
    "    preprocess_function=partial(preprocess_fn, tokenizer=quantizer.tokenizer),\n",
    "    num_samples=calibration_samples,\n",
    "    dataset_split=\"train\",\n",
    ")\n",
    "\n",
    "# Create the calibration configuration containing the parameters related to calibration.\n",
    "calibration_config = AutoCalibrationConfig.percentiles(calibration_dataset, percentile=99.99239080907178)\n",
    "\n",
    "# Perform the calibration step: computes the activations quantization ranges\n",
    "shards=16\n",
    "for i in range(shards):\n",
    "    shard = calibration_dataset.shard(shards, i)\n",
    "    quantizer.partial_fit(\n",
    "        dataset=shard,\n",
    "        calibration_config=calibration_config,\n",
    "        onnx_model_path=onnx_path / \"model.onnx\",\n",
    "        operators_to_quantize=qconfig.operators_to_quantize,\n",
    "        batch_size=calibration_samples//shards,\n",
    "        use_external_data_format=False,\n",
    "    )\n",
    "ranges = quantizer.compute_ranges()\n",
    "\n",
    "# remove temp augmented model again\n",
    "os.remove(\"augmented_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use the ORTQuantizer to apply static quantization\n",
    "\n",
    "After we have calculated our calibration tensor ranges we can quantize our model using the `ORTQuantizer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx/model-quantized.onnx')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import create_quantization_preprocessor\n",
    "\n",
    "# create processor\n",
    "quantization_preprocessor = create_quantization_preprocessor()\n",
    "\n",
    "# Quantize the same way we did for dynamic quantization!\n",
    "quantizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_quantized_model_output_path=onnx_path / \"model-quantized.onnx\",\n",
    "    calibration_tensors_range=ranges,\n",
    "    quantization_config=qconfig,\n",
    "    preprocessor=quantization_preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly check the new model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: 255.68 MB\n",
      "Quantized Model file size: 134.32 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "quantized_model = os.path.getsize(onnx_path / \"model-quantized.onnx\")/(1024*1024)\n",
    "\n",
    "print(f\"Model file size: {size:.2f} MB\")\n",
    "print(f\"Quantized Model file size: {quantized_model:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test inference with the quantized model\n",
    "\n",
    "[Optimum](https://huggingface.co/docs/optimum/main/en/pipelines#optimizing-with-ortoptimizer) has built-in support for [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines). This allows us to leverage the same API that we know from using PyTorch and TensorFlow models.\n",
    "Therefore we can load our quantized model with `ORTModelForSequenceClassification` class and transformers `pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'exchange_rate', 'score': 0.9802021384239197}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(onnx_path,file_name=\"model-quantized.onnx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_path)\n",
    "\n",
    "clx = pipeline(\"text-classification\",model=model, tokenizer=tokenizer)\n",
    "\n",
    "clx(\"What is the exchange rate like on this app?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the performance and speed\n",
    "\n",
    "We can now leverage the map function of datasets to iterate over the validation set of squad 2 and run prediction for each data point. Therefore we write a evaluate helper method which uses our pipelines and applies some transformation to work with the squad v2 metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n",
      "Couldn't find a directory or a metric named 'accuracy' in this version. It was picked from the master branch on github instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9224025974025974}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "from datasets import load_dataset \n",
    "\n",
    "eval = evaluator(\"text-classification\")\n",
    "eval_dataset = load_dataset(\"banking77\", split=\"test\")\n",
    "\n",
    "results = eval.compute(\n",
    "    model_or_pipeline=clx,\n",
    "    data=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    input_column=\"text\",\n",
    "    label_column=\"label\",\n",
    "    label_mapping=model.config.label2id,\n",
    "    strategy=\"simple\",\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model: 92.5%\n",
      "Quantized model: 92.24%\n",
      "The quantized model achieves 99.72% accuracy of the fp32 model\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vanilla model: 92.5%\")\n",
    "print(f\"Quantized model: {results['accuracy']*100:.2f}%\")\n",
    "print(f\"The quantized model achieves {round(results['accuracy']/0.925,4)*100:.2f}% accuracy of the fp32 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's test the performance (latency) of our quantized model. We are going to use a payload with a sequence length of 128 for the benchmark. To keep it simple, we are going to use a python loop and calculate the avg,mean & p95 latency for our vanilla model and for the quantized model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length: 128\n",
      "Vanilla model: P95 latency (ms) - 75.69221085868777; Average latency (ms) - 57.52 +\\- 6.16;\n",
      "Quantized model: P95 latency (ms) - 26.75848939397838; Average latency (ms) - 24.86 +\\- 1.25;\n",
      "Improvement through quantization: 2.83x\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "\n",
    "payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend \"*2\n",
    "print(f'Payload sequence length: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(payload)\n",
    "    # Timed run\n",
    "    for _ in range(300):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(payload)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "vanilla_clx = pipeline(\"text-classification\",model=model_id)\n",
    "\n",
    "\n",
    "vanilla_model=measure_latency(vanilla_clx)\n",
    "quantized_model=measure_latency(clx)\n",
    "\n",
    "print(f\"Vanilla model: {vanilla_model[0]}\")\n",
    "print(f\"Quantized model: {quantized_model[0]}\")\n",
    "print(f\"Improvement through quantization: {round(vanilla_model[1]/quantized_model[1],2)}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to accelerate our model latency from 75.69ms to 26.75ms or 2.83x while keeping 99.72% of the accuracy. \n",
    "\n",
    "![performance](images/static-quantization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Push the quantized model to the Hub\n",
    "\n",
    "The Optimum model classes like `ORTModelForSequenceClassification` are integrated with the Hugging Face Model Hub, which means you can not only load model from the Hub, but also push your models to the Hub with `push_to_hub()` method. That way we can now save our qunatized model on the Hub to be for example used inside our inference API.\n",
    "\n",
    "_We have to make sure that we are also saving the `tokenizer` as well as the `config.json` to have a good inference experience._\n",
    "\n",
    "If you haven't logged into the `huggingface hub` yet you can use the `notebook_login` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have configured our hugging face hub credentials we can push the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.9/site-packages/huggingface_hub/hf_api.py:79: FutureWarning: `name` and `organization` input arguments are deprecated and will be removed in v0.8. Pass `repo_id` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "tmp_store_directory=\"onnx_hub_repo\"\n",
    "repository_id=\"quantized-distilbert-banking77\"\n",
    "model_file_name=\"model-quantized.onnx\"\n",
    "\n",
    "model.latest_model_name=model_file_name # workaround for PR #214\n",
    "model.save_pretrained(tmp_store_directory)\n",
    "quantizer.tokenizer.save_pretrained(tmp_store_directory)\n",
    "\n",
    "model.push_to_hub(tmp_store_directory,\n",
    "                  repository_id=repository_id,\n",
    "                  use_auth_token=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load and run inference with a quantized model from the hub\n",
    "\n",
    "This step serves as a demonstration of how you could use optimum in your api to load and use our qunatized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.78k/5.78k [00:00<00:00, 3.48MB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141M/141M [00:01<00:00, 112MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'exchange_rate', 'score': 0.9802021384239197}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"philschmid/quantized-distilbert-banking77\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/quantized-distilbert-banking77\")\n",
    "\n",
    "remote_clx = pipeline(\"text-classification\",model=model, tokenizer=tokenizer)\n",
    "\n",
    "remote_clx(\"What is the exchange rate like on this app?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5 ms Â± 338 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "remote_clx(\"What is the exchange rate like on this app?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We successfully quantized our vanilla Transformers model with Hugging Face and managed to accelerate our model latency from 75.69ms to 26.75ms or 2.83x while keeping 99.72% of the accuracy. \n",
    "\n",
    "But this i have to say that this isn't a plug and play process you can transfer to any Transformers model, task and dataset. The challenge with static quantization ist the calibration of the dataset to find the right ranges which you can use to quantize the model achieve good performance. I ran a hyperparameter search to find the best ranges for our dataset and quantized model using the [run_static_quantizatio_hpo.py](https://github.com/philschmid/optimum-static-quantization/blob/master/scripts/run_static_quantizatio_hpo.py). \n",
    "\n",
    "Also noteably to say it that static quantization can only achieve as good as results as dynamic quantization, but will be faster than dynamic quantization. Meaning that it might always be a good start to first dynamically quantize your model using Optimum and then move to static quantization for further latency and throughput gains. The attached repository also includes an example on how dynamically quantize the model [dynamic_quantization.py](https://github.com/philschmid/optimum-static-quantization/blob/master/scripts/dynamic_quantization.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a2c4b191d1ae843dde5cb5f4d1f62fa892f6b79b0f9392a84691e890e33c5a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
