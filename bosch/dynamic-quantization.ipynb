{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a105dca0-1688-44c1-84fd-0bded528ca51",
      "metadata": {},
      "source": [
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/huggingface/workshops/blob/main/bosch/dynamic-quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee47bc31-6686-4cf4-b81d-1ac6b0a99a05",
      "metadata": {},
      "source": [
        "# Dynamic Quantization with Hugging Face Optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c100e377-0f26-4966-a76c-d4e1f5dfe3de",
      "metadata": {},
      "source": [
        "In this session, you will learn how to apply _dynamic quantization_ to a \ud83e\udd17 Transformers model. You will quantize a [DistilBERT model](https://huggingface.co/optimum/distilbert-base-uncased-finetuned-banking77) that's been fine-tuned on the [Banking77 dataset](https://huggingface.co/datasets/banking77) for intent classification. \n",
        "\n",
        "Along the way, you'll learn how to use two open-source libraries: \n",
        "\n",
        "* [\ud83e\udd17 Optimum](https://github.com/huggingface/optimum): an extension of \ud83e\udd17 Transformers, which provides a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware.\n",
        "* [\ud83e\udd17 Evaluate](https://github.com/huggingface/evaluate): a library that makes evaluating and comparing models and reporting their performance easier and more standardized.\n",
        "\n",
        "\n",
        "By the end of this session, you see how graph optimization and quantization with \ud83e\udd17 Optimum can significantly decrease model latency while keeping almost 100% of the full-precision model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4149dbcf-508c-4c70-b794-91376a7e3662",
      "metadata": {},
      "source": [
        "## Learning objectives\n",
        "\n",
        "By the end of this session, you will know how to:\n",
        "\n",
        "* Setup a development environment\n",
        "* Convert a \ud83e\udd17 Transformers model to ONNX for inference\n",
        "* Apply dynamic quantization using `ORTQuantizer` from \ud83e\udd17 Optimum\n",
        "* Test inference with the quantized model\n",
        "* Evaluate the model performance with \ud83e\udd17 Evaluate\n",
        "* Compare the latency of the quantized model against the original one\n",
        "* Push the quantized model to the Hub\n",
        "* Load and run inference with a quantized model from the Hub\n",
        "\n",
        "\n",
        "Let's get started! \ud83d\ude80"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0795d087-77f9-4d82-915f-983f13abf4ea",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 1. Setup development environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72bdd46b-9161-4a09-acfa-cbcd095f55e2",
      "metadata": {},
      "source": [
        "Our first step is to install \ud83e\udd17 Optimum, along with \ud83e\udd17 Evaluate and some other libraries. Running the following cell will install all the required packages for us including \ud83e\udd17 Transformer, PyTorch, and ONNX Runtime utilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "327dfc3f-02be-4816-bd20-305607accef0",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install \"optimum[onnxruntime]\" \"evaluate[evaluator]\" sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90e16fc-e9bb-41f0-aa50-948a3c4c7d25",
      "metadata": {},
      "source": [
        "> If you want to run inference on a GPU, you can install \ud83e\udd17 Optimum with `pip install optimum[onnxruntime-gpu]`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b84656-0248-4b39-a8cb-65bd3b615a0d",
      "metadata": {},
      "source": [
        "While we're at it, let's turn off some of the warnings from the \ud83e\udd17 Datasets library and the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1c8841c5-79c6-4ff7-936f-832faaf706cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TOKENIZERS_PARALLELISM=false\n"
          ]
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "datasets.logging.set_verbosity_error()\n",
        "\n",
        "%env TOKENIZERS_PARALLELISM=false"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a977d3-9006-47a7-aefb-6d5ea79c00f8",
      "metadata": {},
      "source": [
        "## 2. Convert a \ud83e\udd17 Transformers model to ONNX for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9680fe1b-e176-49ed-8ed6-f0ec35049d2b",
      "metadata": {},
      "source": [
        "Before we can optimize and quantize our model, we first need to export it to the ONNX format. To do this we will use the `ORTModelForSequenceClassification` class and call the `from_pretrained()` method. This method will download the PyTorch weights from the Hub and export them via the `from_transformers` argument. The model we are using is `optimum/distilbert-base-uncased-finetuned-banking77`, which is a fine-tuned DistilBERT model on the Banking77 dataset achieving an accuracy score of 91.7% and as the feature (task) text-classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0866e06f-cadc-412c-9bc4-ab7c77d84e3b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25c01bab3adf46b6a6f50c294e9dbd58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/5.84k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f98041199bf46d5aeaaebb473f8b0c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/346 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02acd67c2b0d43d4bb3337151da2a7e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48865fb4133e4ad58531e86d59b4dd37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cb95de586714834aa3d79c770203fa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8b6493ede384246af113fe5a077b0f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"lewtun/autotrain-sphere-banking77-1565555714\"\n",
        "dataset_id = \"banking77\"\n",
        "onnx_path = Path(\"onnx\")\n",
        "\n",
        "# load vanilla transformers and convert to onnx\n",
        "model = ORTModelForSequenceClassification.from_pretrained(\n",
        "    model_id, from_transformers=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3913851-68c9-45ee-b569-d68c47388ced",
      "metadata": {},
      "source": [
        "One neat thing about \ud83e\udd17 Optimum, is that allows you to run ONNX models with the `pipeline()` function from  \ud83e\udd17 Transformers. This means that you get all the pre- and post-processing features for free, without needing to re-implement them for each model! Here's how you can run inference with our vanilla ONNX model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5930fe08-a929-42f5-989c-95c826cbfd6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "vanilla_clf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "vanilla_clf(\"Could you assist me in finding my lost card?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b143952-97dd-4b5e-b494-439fee87078c",
      "metadata": {},
      "source": [
        "This looks good, so let's save the model and tokenizer to disk for later usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d012c47b-9018-4f28-b6aa-d83d3b011514",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('onnx/tokenizer_config.json',\n",
              " 'onnx/special_tokens_map.json',\n",
              " 'onnx/vocab.txt',\n",
              " 'onnx/added_tokens.json',\n",
              " 'onnx/tokenizer.json')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# save onnx checkpoint and tokenizer\n",
        "model.save_pretrained(onnx_path)\n",
        "tokenizer.save_pretrained(onnx_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05e9d6dd-b1de-4f7c-92e6-7857c443e29c",
      "metadata": {},
      "source": [
        "If we inspect the `onnx` directory where we've saved the model and tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c647267e-c05a-4f3a-b80f-afe24de6ebfd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json  special_tokens_map.json  tokenizer_config.json\n",
            "model.onnx   tokenizer.json\t      vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls {onnx_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "663e0138-b942-4d15-a98a-345de6d3b286",
      "metadata": {},
      "source": [
        "we can see that there's a `model.onnx` file that corresponds to our exported model. Let's now go ahead and optimize this!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fa336cf-fdcf-42bb-805f-05b763d7e3be",
      "metadata": {},
      "source": [
        "## 3. Apply graph optimization using `ORTOptimizer` from \ud83e\udd17 Optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "656fc904-e2a1-4bc8-ae03-b8ea3bebe9f6",
      "metadata": {},
      "source": [
        "To apply graph optimization in \ud83e\udd17 Optimum, we do this by:\n",
        "\n",
        "* Creating an optimizer based on our ONNX model\n",
        "* Defining the type of optimizations via a configuration class\n",
        "* Exporting the optimized model as a new ONNX file\n",
        "\n",
        "The following code snippet does these steps for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ff5d49d-c124-4b06-841f-8b45d3fa11d0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('onnx')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from optimum.onnxruntime import ORTOptimizer\n",
        "from optimum.onnxruntime.configuration import OptimizationConfig\n",
        "\n",
        "optimizer = ORTOptimizer.from_pretrained(model)\n",
        "\n",
        "optimization_config = OptimizationConfig(optimization_level=2,\n",
        "    optimize_with_onnxruntime_only=False,\n",
        "    optimize_for_gpu=False,\n",
        ")\n",
        "\n",
        "optimizer.optimize(save_dir=onnx_path, optimization_config=optimization_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e64bf2b8-506f-4eb1-bd85-19f05c624774",
      "metadata": {},
      "source": [
        "Here we can see that we've specifed in the configuration which level of optimisation to apply, along with optimizing for CPU only. If we now take a look at our `onnx` directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d1336da9-dde1-47df-9da3-553450b84cc6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json\t      ort_config.json\t       tokenizer_config.json\n",
            "model.onnx\t      special_tokens_map.json  vocab.txt\n",
            "model_optimized.onnx  tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls {onnx_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f52c0194-c04c-4611-ad4d-10735c25d9db",
      "metadata": {},
      "source": [
        "we can see we have a new ONNX file called `model_optimized.onnx`. Let's do a quick speed test of the two models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5ecb24-2368-42fc-b43b-db841a0cfdf2",
      "metadata": {},
      "source": [
        "## 4. Test inference with the optimized model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb3cc5e-eca8-4d60-abc0-e276fa903b0c",
      "metadata": {},
      "source": [
        "As we saw earlier, Optimum has built-in support for transformers pipelines. This allows us to leverage the same API that we know from using PyTorch and TensorFlow models. Therefore we can load our quantized model with `ORTModelForSequenceClassification` class and the transformers `pipeline()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7c397d37-faa3-43f1-b03a-4f8dd4f2eebb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'lost_or_stolen_card', 'score': 0.9500694870948792}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_optimized = ORTModelForSequenceClassification.from_pretrained(\n",
        "    onnx_path, file_name=\"model_optimized.onnx\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(onnx_path)\n",
        "\n",
        "optimized_clf = pipeline(\"text-classification\", model=model_optimized, tokenizer=tokenizer)\n",
        "optimized_clf(\"Could you assist me in finding my lost card?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c78133-949b-4c01-91d3-bde8a55d93b6",
      "metadata": {},
      "source": [
        "## 5. Compare the latency of the optimized model against the original one"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9c915a-ef15-4d2e-ab40-3ffbcd3f206e",
      "metadata": {},
      "source": [
        "Okay, now let's test the performance (latency) of our optimized model. We are going to use a payload with a sequence length of 128 for the benchmark. To keep it simple, we are going to use a Python loop and calculate the avgerage and p95 latencies for our vanilla model and for the optimized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5234d5b8-edb4-412c-9fb3-c4442c37be7a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Payload sequence length: 128\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "999108d7589240d896b8acf349489528",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba6f37606e4749cd86c89da33732f354",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vanilla model: P95 latency (ms) - 40.66422965042875; Average latency (ms) - 37.75 +\\- 2.26;\n",
            "Optimized model: P95 latency (ms) - 35.47789009908229; Average latency (ms) - 32.11 +\\- 2.59;\n",
            "Improvement through quantization: 1.15x\n"
          ]
        }
      ],
      "source": [
        "from time import perf_counter\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "payload = (\n",
        "    \"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend \"\n",
        "    * 2\n",
        ")\n",
        "print(f'Payload sequence length: {len(tokenizer(payload)[\"input_ids\"])}')\n",
        "\n",
        "\n",
        "def measure_latency(pipe):\n",
        "    latencies = []\n",
        "    # warm up\n",
        "    for _ in range(10):\n",
        "        _ = pipe(payload)\n",
        "    # Timed run\n",
        "    for _ in tqdm(range(300)):\n",
        "        start_time = perf_counter()\n",
        "        _ = pipe(payload)\n",
        "        latency = perf_counter() - start_time\n",
        "        latencies.append(latency)\n",
        "    # Compute run statistics\n",
        "    time_avg_ms = 1000 * np.mean(latencies)\n",
        "    time_std_ms = 1000 * np.std(latencies)\n",
        "    time_p95_ms = 1000 * np.percentile(latencies, 95)\n",
        "    return (\n",
        "        f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\",\n",
        "        time_p95_ms,\n",
        "    )\n",
        "\n",
        "\n",
        "vanilla_latencies = measure_latency(vanilla_clf)\n",
        "optimized_latencies = measure_latency(optimized_clf)\n",
        "\n",
        "print(f\"Vanilla model: {vanilla_latencies[0]}\")\n",
        "print(f\"Optimized model: {optimized_latencies[0]}\")\n",
        "print(\n",
        "    f\"Improvement through quantization: {round(vanilla_latencies[1]/optimized_latencies[1],2)}x\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39605cd9-bb79-4cab-aaa1-81dd7ce5691d",
      "metadata": {},
      "source": [
        "Nice, applying graph optimization has given us a decent speed up! Let's see if we can squeeze a bit more performance with quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60b2a17-d71b-47a8-b4f3-2e6b5d05dd4d",
      "metadata": {
        "tags": []
      },
      "source": [
        "## 6. Apply dynamic quantization using `ORTQuantizer` from \ud83e\udd17 Optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9c07243-2c83-4443-a217-7e460789bcd1",
      "metadata": {},
      "source": [
        "To apply quantization in \ud83e\udd17 Optimum, we do this by:\n",
        "\n",
        "* Creating an optimizer based on our ONNX model\n",
        "* Defining the type of optimizations via a configuration class\n",
        "* Exporting the optimized model as a new ONNX file\n",
        "\n",
        "The following code snippet does these steps for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d9f5bbe9-bf45-4f4b-8a62-4607bfd89c8d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('onnx')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from optimum.onnxruntime import ORTQuantizer\n",
        "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
        "\n",
        "# create ORTQuantizer and define quantization configuration\n",
        "model_optimized = ORTModelForSequenceClassification.from_pretrained(onnx_path, file_name=\"model_optimized.onnx\")\n",
        "dynamic_quantizer = ORTQuantizer.from_pretrained(model_optimized)\n",
        "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
        "\n",
        "# apply the quantization configuration to the model\n",
        "dynamic_quantizer.quantize(save_dir=onnx_path, quantization_config=dqconfig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac9ed27-261b-4517-8612-9f5cb3d7fc12",
      "metadata": {},
      "source": [
        "Here we can see that we've specifed in the configuration the type of execution engine to use with the Intel AVX512-VNNI CPU. If we now take a look at our `onnx` directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "857de870-66a4-4047-bae0-80ad8ab51512",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.json\t      model_optimized_quantized.onnx  tokenizer.json\n",
            "model.onnx\t      ort_config.json\t\t      tokenizer_config.json\n",
            "model_optimized.onnx  special_tokens_map.json\t      vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls {onnx_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bdea9b-1a03-42c4-accf-a0bfcf4b115b",
      "metadata": {},
      "source": [
        "we can see we have a new ONNX file called `model_optimized_quantized.onnx`. Let's do a quick size comparison of the two models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bd67a815-421d-4b0a-a41e-28cdb11ea256",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model file size: 255.68 MB\n",
            "Quantized Model file size: 162.68 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# get model file size\n",
        "size = os.path.getsize(onnx_path / \"model.onnx\") / (1024 * 1024)\n",
        "quantized_model = os.path.getsize(onnx_path / \"model_optimized_quantized.onnx\") / (1024 * 1024)\n",
        "\n",
        "print(f\"Model file size: {size:.2f} MB\")\n",
        "print(f\"Quantized Model file size: {quantized_model:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff232e15-5baa-4d00-b7bb-93407144889f",
      "metadata": {},
      "source": [
        "Nice, dynamic quantization has reduced the model size by around a factor of 2! This should allow us to speed up the inference time by a similar factor, so let's now see how we can test the latency of our models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181a828c-fc79-403c-b2e0-305a2f0302bb",
      "metadata": {},
      "source": [
        "## 7. Test inference with the quantized model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86af0f26-fbf4-4016-81ff-c822ff6383c1",
      "metadata": {},
      "source": [
        "As before, the first order of business is to create a new pipeline for our quantized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b2bdc068-d03f-41b8-979b-843759885099",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'lost_or_stolen_card', 'score': 0.9163928627967834}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_quantized = ORTModelForSequenceClassification.from_pretrained(\n",
        "    onnx_path, file_name=\"model_optimized_quantized.onnx\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(onnx_path)\n",
        "\n",
        "quantized_clf = pipeline(\"text-classification\", model=model_quantized, tokenizer=tokenizer)\n",
        "quantized_clf(\"Could you assist me in finding my lost card?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e74443e-af85-4898-84aa-acc764977c88",
      "metadata": {},
      "source": [
        "## 8. Compare the latency of the quantized model against the original one"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aca1d748-0fb5-4d23-9ac4-0a5052d3da79",
      "metadata": {},
      "source": [
        "Okay, now let's test the performance (latency) of our quantized model. We are going to use a payload with a sequence length of 128 for the benchmark. To keep it simple, we are going to use a Python loop and calculate the avgerage and p95 latencies for our vanilla model and for the quantized model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "db0ecd12-b9a6-44f5-b4e3-3984ff8529e1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b87dd7eb71fa40e8b3b881768b45cc0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vanilla model: P95 latency (ms) - 40.66422965042875; Average latency (ms) - 37.75 +\\- 2.26;\n",
            "Quantized model: P95 latency (ms) - 27.898280650788365; Average latency (ms) - 25.40 +\\- 1.82;\n",
            "Improvement through quantization: 1.46x\n"
          ]
        }
      ],
      "source": [
        "quantized_latencies = measure_latency(quantized_clf)\n",
        "\n",
        "print(f\"Vanilla model: {vanilla_latencies[0]}\")\n",
        "print(f\"Quantized model: {quantized_latencies[0]}\")\n",
        "print(\n",
        "    f\"Improvement through quantization: {round(vanilla_latencies[1]/quantized_latencies[1],2)}x\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b0ae56-b9bb-405e-af8b-31bd52efad10",
      "metadata": {},
      "source": [
        "Nice, our model is model is a bit over two times faster! Let's see what the impacty on accuracy is"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff9240a-97dd-4bc1-b8e4-d09f442d4e99",
      "metadata": {},
      "source": [
        "## 9. Evaluate the model performance with \ud83e\udd17 Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dbffd76-bd34-4459-bf8d-f5b64c13d858",
      "metadata": {},
      "source": [
        "It is always a good idea to evaluate the performance of your quantized model on a dedicated test set to ensure the optimizations haven't impacted the model too strongly. To evaluate our model, we'll use the handy `evaluator()` function from \ud83e\udd17 Evaluate. This function is similar to the `pipeline()` function from \ud83e\udd17 Transformers, in the sense that it handles the evaluation loop for you automatically!\n",
        "\n",
        "Here's how you can load an evaluator for text classification and feed in the quantized pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5e18775e-3e34-4d94-8643-865fa402e56c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accuracy': 0.912987012987013, 'total_time_in_seconds': 16.234970625000642, 'samples_per_second': 189.71392502903763, 'latency_in_seconds': 0.005271094358766442}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from evaluate import evaluator\n",
        "\n",
        "eval_pipe = evaluator(\"text-classification\")\n",
        "eval_dataset = load_dataset(dataset_id, split=\"test\")\n",
        "label_feature = eval_dataset.features[\"label\"]\n",
        "label2id = {label_feature.int2str(idx):idx for idx in range(label_feature.num_classes)}\n",
        "\n",
        "results = eval_pipe.compute(\n",
        "    model_or_pipeline=quantized_clf,\n",
        "    data=eval_dataset,\n",
        "    metric=\"accuracy\",\n",
        "    input_column=\"text\",\n",
        "    label_column=\"label\",\n",
        "    label_mapping=label2id,\n",
        "    strategy=\"simple\",\n",
        ")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1036ccbd-3210-4c13-9a21-ff7ee0540f0e",
      "metadata": {},
      "source": [
        "Not bad! The resulting accuracy isn't too far from the original model - let's see how much exactly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c076191e-555f-4ac7-ae11-22414ce99cc5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vanilla model: 91.68%\n",
            "Quantized model: 91.30%\n",
            "The quantized model achieves 98.70% accuracy of the fp32 model\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vanilla model: 91.68%\")\n",
        "print(f\"Quantized model: {results['accuracy']*100:.2f}%\")\n",
        "print(\n",
        "    f\"The quantized model achieves {round(results['accuracy']/0.925,4)*100:.2f}% accuracy of the fp32 model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc4c920-009f-451c-8455-cb7b96c21783",
      "metadata": {},
      "source": [
        "## 10. Push the quantized model to the Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501f2de8-044f-4c99-acad-d9386171b863",
      "metadata": {},
      "source": [
        "The Optimum model classes like `ORTModelForSequenceClassification` are integrated with the Hugging Face Model Hub, which means you can not only load model from the Hub, but also push your models to the Hub with the `push_to_hub()` method. That way we can now save our qunatized model on the Hub to be for example used inside our inference API.\n",
        "\n",
        "We have to make sure that we are also saving the tokenizer as well as the `config.json` to have a good inference experience.\n",
        "\n",
        "If you haven't logged into the Hub yet you can use the `notebook_login` function to do so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b8682b6b-21ac-4c1a-ab4f-d7ac602e0dd1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29a37271d87b4a5b8ba7c4fababf8a0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4824749-a2f7-4d4f-ac98-4c4dc5d15988",
      "metadata": {},
      "source": [
        "It's then a simple mater of saving our files to a local directory and running the `push_to_hub()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "79bfddd0-7133-404a-b15b-5b965476ecf9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lewis_huggingface_co/miniconda3/envs/sphere/lib/python3.8/site-packages/huggingface_hub/hf_api.py:102: FutureWarning: `name` and `organization` input arguments are deprecated and will be removed in v0.10. Pass `repo_id` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tmp_store_directory = \"onnx_hub_repo\"\n",
        "repository_id = \"quantized-distilbert-banking77\"\n",
        "\n",
        "model.save_pretrained(tmp_store_directory)\n",
        "tokenizer.save_pretrained(tmp_store_directory)\n",
        "\n",
        "model.push_to_hub(tmp_store_directory, repository_id=repository_id, use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6697b734-fc70-4abb-916d-abee8c2e30ae",
      "metadata": {},
      "source": [
        "## 11. Load and run inference from the Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0348b5fa-283b-4d1b-ac35-633f05905ba4",
      "metadata": {},
      "source": [
        "Now that our model is on the Hub, we can use it from anywhere! Here's a demo to show how we can load the model and tokenizer, before passing them to the `pipeline()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1ec42230-b021-40f4-9879-d377f3975e31",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7b94f01a21749dcbe05972132f3b14e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/5.84k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6eb290ba6dc3453b85129dc96e86346c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ef3a1fc5bd74156b0e09e5d3ede3a31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/341 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61b1ae428d6342f386deec84450a9a9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a16bfb6e8a740828781a54e613ff87d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3d2664f7667460c8577200426c3f825",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'lost_or_stolen_card', 'score': 0.9500694870948792}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ORTModelForSequenceClassification.from_pretrained(\n",
        "    \"lewtun/quantized-distilbert-banking77-2\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lewtun/quantized-distilbert-banking77-2\")\n",
        "\n",
        "remote_clf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "remote_clf(\"Could you assist me in finding my lost card?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2851e8c0-cb85-40a0-b216-e02d7f121b0a",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hf",
      "language": "python",
      "name": "hf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0e16f9ee4ecdbf0dfbe29801ff22c01c130eb8d5db28058daa1c23f86d59ede5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}