{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f21dd773",
      "metadata": {},
      "source": [
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/huggingface/workshops/blob/main/luzern-university/06-stable-diffusion-gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c5db8ce-b7a1-4c4c-a1c3-6933efd1d186",
      "metadata": {},
      "source": [
        "# Creating a Diffusers Demo with Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "892a3b27",
      "metadata": {},
      "source": [
        "**Learning goals:** The goal of this tutorial is to learn How To\n",
        "\n",
        "1. Build a quick demo for your machine learning model in Python using the `gradio` library\n",
        "2. Host the demos for free with Hugging Face Spaces\n",
        "\n",
        "**Duration**: 20-40\n",
        " minutes\n",
        "\n",
        "**Prerequisites:** Knowledge of Python and basic familiarity with machine learning\n",
        "\n",
        "All of these steps can be done for free! All you need is an Internet browser and a place where you can write Python \ud83d\udc69\u200d\ud83d\udcbb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27b8c03d",
      "metadata": {},
      "source": [
        "## Why Demos?\n",
        "\n",
        "**Demos** of machine learning models are an increasingly important part of machine learning _courses_ and _conferences_. Demos allow:\n",
        "\n",
        "* model developers to easily **present** their work to a wide audience\n",
        "* increase **reproducibility** of machine learning research\n",
        "* diverse users to more easily **identify and debug** failure points of models\n",
        "\n",
        "\n",
        "As a quick example of what we would like to build, check out the [Keras Org on Hugging Face](https://huggingface.co/keras-io), which includes a description card and a collection of Models and Spaces built by Keras community. Any Space can be opened in your browser and you can use the model immediately, as shown here: \n",
        "\n",
        "![](https://i.ibb.co/7y6DGjB/ezgif-5-cc52b7e590.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8cf951-41e2-49a9-b2a7-fb3ea8019c64",
      "metadata": {
        "tags": []
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1469f27a-1e94-4e54-8cb3-41c9387cbb59",
      "metadata": {},
      "source": [
        "If you're running this notebook on Google Colab or locally, you'll need a few dependencies installed. You can install them with `pip` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a51177-60dc-4641-be71-ff05f067f738",
      "metadata": {},
      "outputs": [],
      "source": [
        "#! pip install diffusers transformers ftfy gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7741507d-1b34-41cc-bedb-4fd135817f68",
      "metadata": {},
      "source": [
        "To be able to access the Stable Diffusion checkpoints from the Hugging Face Hub, you'll need to store your authentication token from the Hugging Face website. Sign up [here](https://huggingface.co/join) if you haven't already, then execute the following cell and input your API token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9432d2-911b-4587-aa58-c4b42ab0900f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec400261-f810-41ac-a7fb-3c77ababd611",
      "metadata": {},
      "source": [
        "Finally, let's set the device (CPU or GPU) to run our pipelines on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca1a1399-9d75-417e-b4ec-07724edfe2b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if device == \"cuda\" else None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58335633-23dc-46a3-a8b0-73f14d0727ec",
      "metadata": {},
      "source": [
        "## Building a demo with Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab2bbd06",
      "metadata": {},
      "source": [
        "`gradio` is a handy Python library that lets you build web demos simply by specifying the list of input and output **components** expected by your machine learning model. \n",
        "\n",
        "What do I mean by input and output components? Gradio comes with a bunch of predefined components for different kinds of machine learning models. Here are some examples:\n",
        "\n",
        "* For an **image classifier**, the expected input type is an `Image` and the output type is a `Label`. \n",
        "* For a **speech recognition model**, the expected input component is an `Microphone` (which lets users record from the browser) or `Audio` (which lets users drag-and-drop audio files), while the output type is `Text`. \n",
        "* For a **question answering model**, we expect **2 inputs**: [`Text`, `Text`], one textbox for the paragraph and one for the question, and the output type is a single `Text` corresponding to the answer. \n",
        "\n",
        "You get the idea... (for all of the supported components, [see the docs](https://gradio.app/docs/))\n",
        "\n",
        "In addition to the input and output types, Gradio expects a third parameter, which is the prediction function itself. This parameter can be ***any* regular Python function** that takes in parameter(s) corresponding to the input component(s) and returns value(s) corresponding to the output component(s)\n",
        "\n",
        "Enough words. Let's see some code!\n",
        "\n",
        "First, let's load the `StableDiffusionPipeline` and check it can generate an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce5b462f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_id, revision=\"fp16\", torch_dtype=torch_dtype\n",
        ").to(device)\n",
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "\n",
        "outputs = pipe(prompt)\n",
        "outputs.image[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bedd0ad1",
      "metadata": {},
      "source": [
        "Next, we need to implement a function that takes a text prompt and returns an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b9087d78-5da7-4ed0-882a-401ff32ae407",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(prompt):\n",
        "    return pipe(prompt).images[0]\n",
        "\n",
        "\n",
        "predict(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df63195c-dda8-4418-a938-79b297e47a0c",
      "metadata": {},
      "source": [
        "The final step is to implement a simple interface:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd091b0-4843-4fb3-8192-b5b705d7171a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "gradio_ui = gr.Interface(\n",
        "    fn=predict,\n",
        "    title=\"Stable Diffusion Demo\",\n",
        "    description=\"Enter a description of an image you'd like to generate!\",\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, label=\"Paste some text here\"),\n",
        "    ],\n",
        "    outputs=[\"image\"],\n",
        "    examples=[[\"a photograph of an astronaut riding a horse\"]],\n",
        ")\n",
        "\n",
        "gradio_ui.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7e85e6f",
      "metadata": {},
      "source": [
        "Running the code above should produce a simple GUI inside this notebook allowing you to type example inputs and see the output returned by your function.\n",
        "\n",
        "Notice that we define an `Interface` using the 3 ingredients mentioned earlier:\n",
        "* A function\n",
        "* Input component(s)\n",
        "* Output component(s)\n",
        "\n",
        "This is a simple example for images, but the same principle holds true for any other kind of data type."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbb717a2",
      "metadata": {},
      "source": [
        "## Host the Demo (for free) on Hugging Face Spaces\n",
        "\n",
        "Once you made a Gradio demo, you can host it permanently on Hugging Spaces very easily:\n",
        "\n",
        "Here are the steps to that (shown in the GIF below):\n",
        "\n",
        "A. First, create a Hugging Face account if you do not already have one, by visiting https://huggingface.co/ and clicking \"Sign Up\"\n",
        "\n",
        "B. Once you are logged in, click on your profile picture and then click on \"New Space\" underneath it to get to this page: https://huggingface.co/new-space\n",
        "\n",
        "C. Give your Space a name and a license. Select \"Gradio\" as the Space SDK, and then choose \"Public\" if you are fine with everyone accessing your Space and the underlying code\n",
        "\n",
        "D. Then you will find a page that provides you instructions on how to upload your files into the Git repository for that Space. You may also need to add a `requirements.txt` file to specify any Python package dependencies.\n",
        "\n",
        "E. Once you have pushed your files, that's it! Spaces will automatically build your Gradio demo allowing you to share it with anyone, anywhere!\n",
        "\n",
        "![GIF](https://huggingface.co/blog/assets/28_gradio-spaces/spaces-demo-finalized.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c5cea9",
      "metadata": {},
      "source": [
        "You can even embed your Gradio demo on any website -- in a blog, a portfolio page, or even in a colab notebook, like I've done with a Pictionary sketch recognition model below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d0d5f0a8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1000\"\n",
              "            height=\"800\"\n",
              "            src=\"https://hf.space/gradioiframe/stabilityai/stable-diffusion/+\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fbb0ffd5340>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(\n",
        "    src=\"https://hf.space/gradioiframe/stabilityai/stable-diffusion/+\",\n",
        "    width=1000,\n",
        "    height=800,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35457fd6",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hf",
      "language": "python",
      "name": "hf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}