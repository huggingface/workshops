{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22c5c66",
   "metadata": {},
   "source": [
    "{{ badge }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5512190-6089-45f6-999f-0f3cd0c07548",
   "metadata": {},
   "source": [
    "# Accelerating Transformers with Hugging Face Optimum\n",
    "\n",
    "Lewis Tunstall (open-source @ Hugging Face)\n",
    "\n",
    "üê¶ `@_lewtun`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29d3e0-a464-4082-bff0-0182db3f32c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaf20df-151e-4260-ba12-c6bfe90eec3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7dbd23-050e-4aa3-ba12-da722f6fa96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e53d3-b56a-43a7-9807-48d01ea2c95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e6104-e455-4b06-85b7-96782c0d6291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fac18b-d473-46fa-af3e-4cb632f771a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b808e27-e3b0-4a36-bd44-cdaa295e2d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1374980-6c13-4a6e-aa29-e23752d0da5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a357469-00a8-43cc-8bfe-f491e367b3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98beeef6-8c32-4588-b298-1298995427da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "def9147f-41c2-4411-b8d7-5039f0df1d58",
   "metadata": {},
   "source": [
    "## Who is Lewis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5ce5e-c30c-47bf-ae76-0af809c1f078",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/about.png\" alt=\"About me\" title=\"Title text\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f44e9c-4aaf-4f34-a165-69c85c9a49ce",
   "metadata": {},
   "source": [
    "* PhD in Physics from University of Adelaide, Australia\n",
    "* Co-author of O'Reilly book [_Natural Language Processing with Transformers_](https://transformersbook.com/)\n",
    "* Co-developer of the **free** [Hugging Face course](https://huggingface.co/course/chapter1/1)\n",
    "* Maintainer of ONNX API in `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fd108-ca54-409c-908c-2efbc541e9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae6037-b173-4ee6-8ba5-c28dad8b9c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08e039-400f-463d-8ae2-befb01afc545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620abf4-7cf4-4fee-b2f0-1e05d6986701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c7f1b5f-f5ba-4571-a794-2fbf697e595b",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* What is Optimum?\n",
    "* Question answering as a case study\n",
    "* Making models faster with quantization\n",
    "* Optimizing inference with ONNX and ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ca919-fb1d-45f7-8871-b9ca78c13e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890031d0-e9c9-4c0a-aaf5-f0c00dc4642e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a6bcd-d778-4891-9736-99b76f42e22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191f88e-27ad-4b0f-8e26-345330f2aff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "860983ce-0a16-45bf-aa96-fc39c6498e2a",
   "metadata": {},
   "source": [
    "## What is Optimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e793d56-9a67-41a9-9236-78b5466d6acf",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/optimum.jpeg\" alt=\"About me\" title=\"Title text\">\n",
    "</div>\n",
    "\n",
    "* An open-source library and extension of Hugging Face Transformers\n",
    "* Provides a unified API of performance optimization tools to achieve maximum efficiency to train and run models on accelerated hardware\n",
    "* Can be used for accelerated training, quantization, graph optimization, and inference with support for `transformers` pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222d57d-14a1-4893-94e4-71eaaf829196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc859a79-b61b-43a3-85cd-c90b5300e365",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/hardware.png\" alt=\"About me\" title=\"Title text\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f55c1-13c1-49e1-a592-821f7d16a932",
   "metadata": {},
   "source": [
    "Today:\n",
    "\n",
    "* Running inference with **ONNX Runtime** in Optimum\n",
    "* Dynamic quantization as a demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d06351-e926-48c0-9b0e-787aaa5bf523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44f4d0ef-a6a6-47bc-bd20-84d67b2b159f",
   "metadata": {},
   "source": [
    "## Question answering as a case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0374c68-4569-455e-acd9-be3681e6e916",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/marie-curie.png\" alt=\"About me\" title=\"Title text\" width=600>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d388663-dbdb-4687-ba0f-db839e5a86c8",
   "metadata": {},
   "source": [
    "* Low latencies critical for user experience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b1b46-4ea0-457e-b5f8-fca0b7ac7e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d0db8f6-c8c0-47ea-bf2f-4ceebef4515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"https://hf.space/gradioiframe/abidlabs/question-answering-simple/+\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0e85c2b640>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.display.IFrame(\"https://hf.space/gradioiframe/abidlabs/question-answering-simple/+\", width=1200, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "192d9d8a-e43f-43d1-bb31-c1265f82941a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7219798564910889, 'start': 277, 'end': 281, 'answer': '1903'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "\n",
    "context = \"\"\"Marie Sklodowska was born in Warsaw, Poland, to a family of teachers who believed strongly in education. She moved to Paris to continue her studies and there met Pierre Curie, who became both her husband and colleague in the field of radioactivity. The couple later shared the 1903 Nobel Prize in Physics. Marie was widowed in 1906, but continued the couple's work and went on to become the first person ever to be awarded two Nobel Prizes. During World War I, Curie organized mobile X-ray teams. The Curies' daughter, Irene, was also jointly awarded the Nobel Prize in Chemistry alongside her husband, Frederic Joliot.\"\"\"\n",
    "question = \"when did marie curie win her first nobel prize?\"\n",
    "pred = question_answerer(question, context)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921fc405-34cc-4760-8158-9d6017a9664c",
   "metadata": {},
   "source": [
    "* Model looks good, deploy to prod?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d3ac2-f8c6-4696-b877-578b33dc7468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df542d6b-02b7-4339-bed5-cf3b24570c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b8491-1490-4785-ab43-bce0f3fcadac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53eafe5a-7654-423a-bf6a-e4d475434125",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/prod.jpeg\" alt=\"About me\" title=\"Title text\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ec027-081d-4cde-a9ad-ad1e639ef11d",
   "metadata": {},
   "source": [
    "Deployment involves tradoff among several constraints:\n",
    "\n",
    "* Model performance (accuracy, F1 score etc)\n",
    "* Latency\n",
    "* Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176c1b3-8a60-4d79-acde-8cf04bf474c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f81d9c14-1f15-47e1-9681-1745b5991ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(question=question, context=context)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(question=question, context=context)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    return f\"average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e39e6f3-d5ea-4a14-ae34-937ab5948ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model average latency (ms) - 96.80 +\\- 0.22\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vanilla model {measure_latency(question_answerer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233f44f-b3ce-4483-8976-49d537c1e549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f907e9-8392-4295-bf77-0f4c88eacb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f89e7bc8-2e93-4765-8bb1-330691b4354a",
   "metadata": {},
   "source": [
    "## Making models faster with quantization\n",
    "\n",
    "Basic idea:\n",
    "\n",
    "* Represent weights and activations with **low-precision data types** like 8-bit integer instead of 32-bit floating point.\n",
    "* Less memory storage & faster matmuls!\n",
    "\n",
    "In practice:\n",
    "\n",
    "* Map range $[f_\\mathrm{min}, f_\\mathrm{max}]$ of floating-point values to smaller range $[q_\\mathrm{min}, q_\\mathrm{max}]$:\n",
    "\n",
    "$$ f = \\left(\\frac{f_\\mathrm{max} - f_\\mathrm{min}}{q_\\mathrm{max} - q_\\mathrm{min}} \\right)(q - Z) = S(q-Z) $$\n",
    "\n",
    "* $S$ is _scale factor_ and $Z$ the _zero point_ (where the quantized value of $f=0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb9a36-1efd-4b14-8ed5-cdd59c552f22",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/fp32-to-int8.png\" alt=\"Quantization\" title=\"Title text\">\n",
    "</div>\n",
    "\n",
    "_Figure courtesy of Manas Sahni_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8776304-7ed8-441c-943c-4f827f8618c7",
   "metadata": {},
   "source": [
    "Three main ways to quantize:\n",
    "\n",
    "* **Dynamic quantization:** quantize weights & activations on-the-fly. Simplest to start with.\n",
    "* **Static quantization:** precompute quantization scheme by observing activation patterns on sample of data. Generally ives better latency, but more complex to calibrate\n",
    "* **Quantization aware training:** simulate quantization during training with \"fake\" quantization of FP32 values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f0449-984e-4af3-be2f-c71f1991d9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "500a472a-f6ce-48d4-93e2-c4932a50835b",
   "metadata": {},
   "source": [
    "## Optimizing inference with ONNX and ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f5a2b-4a7a-4889-90b9-ca125c1109b0",
   "metadata": {},
   "source": [
    "### Step 1: Install Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe36b01-8128-4438-9fb9-0c6e5365f85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84785ee0-451a-45c9-bd51-ddcec03041c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"optimum[onnxruntime]==1.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984eaca-027c-41ed-b042-355663eeb847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a17b5a36-8059-45b8-96ad-04f301dce78e",
   "metadata": {},
   "source": [
    "### Step 2: Export the model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5685f1-c917-4b4c-bcd8-598f152a5e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88c56cfe8e24fc084b06f56e2b205a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('onnx/tokenizer_config.json',\n",
       " 'onnx/special_tokens_map.json',\n",
       " 'onnx/vocab.json',\n",
       " 'onnx/merges.txt',\n",
       " 'onnx/added_tokens.json',\n",
       " 'onnx/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task = \"question-answering\"\n",
    "\n",
    "# Load PyTorch weights and convert to ONNX\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Save ONNX checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9016a2a3-5abe-4e17-be3a-3b36df9ea81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "config.json  model.onnx\t\t   special_tokens_map.json  tokenizer.json\n",
      "merges.txt   model-quantized.onnx  tokenizer_config.json    vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls \"onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541a9e1-ea46-43fc-b61c-eb7b35d47601",
   "metadata": {},
   "source": [
    "### Step 3: Quantize the model with ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b68617-1379-40e6-b28d-bf3753c6c411",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/onnx-ort.png\" alt=\"Quantization\" title=\"Title text\", width=600>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a4ef0-aeb8-405c-8bda-4b8beb1caee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e922c46d-5692-45e6-9973-eda27882c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(model_id, feature=task)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "quantized_path = quantizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_quantized_model_output_path=onnx_path / \"model-quantized.onnx\",\n",
    "    quantization_config=qconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "348466dc-54dc-4845-af36-8eab97a23e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "config.json  model.onnx\t\t   special_tokens_map.json  tokenizer.json\n",
      "merges.txt   model-quantized.onnx  tokenizer_config.json    vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d630192f-65eb-4e24-8a12-83dcd59dfe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Onnx Model file size: 473.34 MB\n",
      "Quantized Onnx Model file size: 230.83 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Vanilla Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model-quantized.onnx\")/(1024*1024)\n",
    "print(f\"Quantized Onnx Model file size: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711966fd-9dcc-4b8d-b525-73689a420198",
   "metadata": {},
   "source": [
    "### Step 4: Run inference with Transformers pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668b8de1-a513-45e8-b17d-1cb50fd96889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ec81e44-7784-4d5f-bc77-68dbb1f33d08",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/cat.png\" alt=\"Quantization\" title=\"Title text\", width=300>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae914d2-fd44-41ba-9f5b-df0417926eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298af6c-0476-4f94-9ce6-d10cd24ad1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad9cd9-9a4d-4ec1-8eb4-3fc373b7632b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f3a95b-93e1-45f6-bc3d-ae8cff75ff06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a13891c-e0fc-498e-b5f5-ca49e0a108bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4670557975769043, 'start': 277, 'end': 281, 'answer': '1903'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load quantized model\n",
    "quantized_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model-quantized.onnx\")\n",
    "\n",
    "# test the quantized model with using transformers pipeline\n",
    "quantized_optimum_qa = pipeline(task, model=quantized_model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = quantized_optimum_qa(question=question, context=context)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15bb048c-570f-4c87-a1ba-3ea5a7a5734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model average latency (ms) - 37.64 +\\- 0.13\n"
     ]
    }
   ],
   "source": [
    "print(f\"Quantized model {measure_latency(quantized_optimum_qa)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4467fec-4634-4d46-a2a5-be74dd0df445",
   "metadata": {},
   "source": [
    "Nice, dynamic quantization gave a ~2x speed-up ü§Ø!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba5184-e7d1-4799-baa7-50ad494188d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90d903-f661-4b5a-87a6-ce293cc5eae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70bb508d-0260-436f-b76f-0326bfd49d36",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dea108a-a3a7-4183-9a42-a3ac8d497359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28712846d57345a7a473aa37785c65f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.logging.set_verbosity_error()\n",
    "from datasets import load_metric, load_dataset\n",
    "\n",
    "metric = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"squad_v2\")[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95247c-27ea-4778-bbc6-ed09d52b7015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06bd06b1-c4d4-4cd6-aa71-b649890b4a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f68206c2b84d90916add2bcbce899c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(example):\n",
    "    default = question_answerer(question=example[\"question\"], context=example[\"context\"])\n",
    "    quantized = quantized_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "    return {\n",
    "      'reference': {'id': example['id'], 'answers': example['answers']},\n",
    "      'default': {'id': example['id'],'prediction_text': default['answer'], 'no_answer_probability': 0.},\n",
    "      'quantized': {'id': example['id'],'prediction_text': quantized['answer'], 'no_answer_probability': 0.},\n",
    "      }\n",
    "\n",
    "result = (dataset\n",
    "          .shuffle(seed=42)    # Shuffle to get a random sample\n",
    "          .select(range(2000)) # Select examples\n",
    "          .map(evaluate))      # Compute model predictions\n",
    "\n",
    "# COMMENT IN to run evaluation on whole validation set - takes ~1.5h\n",
    "# result = dataset.map(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001b488-b577-4f64-a460-ed055125f823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b974a5ca-fae6-4d9f-abd0-9da5a5374ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0a031d4-7428-4270-82bc-316e568ee7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_metrics = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\n",
    "quantized_metrics = metric.compute(predictions=result[\"quantized\"], references=result[\"reference\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5573b93a-713d-4269-8ec8-f95e7e20a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model: Exact match=78.0% F1=81.30079365079364%\n",
      "Quantized model: Exact match=80.0% F1=82.86904761904762%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vanilla model: Exact match={default_metrics['exact']}% F1={default_metrics['f1']}%\")\n",
    "print(f\"Quantized model: Exact match={quantized_metrics['exact']}% F1={quantized_metrics['f1']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e0c51-4e2e-42b8-afb2-7b1a8abdfcfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d56f8-ddf1-4d6e-8de7-0e7dbd1d5662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d041e96-9341-4ba4-944c-19e60beaee2d",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Check out:\n",
    "\n",
    "* the [Optimum repo](https://github.com/huggingface/optimum) and consider giving it a ‚≠êÔ∏è\n",
    "* Philipp Schmid's [blog post](https://huggingface.co/blog/optimum-inference) on the inference features of Optimum\n",
    "* chapter 8 of the O'Reilly [_NLP with Transformers_ book](https://transformersbook.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c247ac-a78f-42c7-972b-71241b1b19ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
