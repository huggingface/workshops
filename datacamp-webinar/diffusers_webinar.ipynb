{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI with ðŸ§¨ diffusers\n",
        "\n",
        "**What is ðŸ§¨ diffusers**?\n",
        "\n",
        "> ðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether youâ€™re looking for a simple inference solution or want to train your own diffusion model, ðŸ¤— Diffusers is a modular toolbox that supports both. Our library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n",
        "\n",
        "\\- from the [docs](https://huggingface.co/docs/diffusers).\n",
        "\n",
        "**Disclaimer**: The materials presented in this notebook are purely intended for educational purposes. Please refer to [our ethical charter](https://huggingface.co/docs/diffusers/main/en/conceptual/ethical_guidelines) for responsibly using the tools shown in this notebook. Also, this notebook is meant to be a hands-on tutorial on Diffusion models with ðŸ§¨ diffusers. For an overview, please refer to [this resource](https://jalammar.github.io/illustrated-stable-diffusion/). \n"
      ],
      "metadata": {
        "id": "Rsa8C-d6Bq5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of contents ðŸ½\n",
        "\n",
        "* Installation and setup\n",
        "* Text-to-image generation in five LoC\n",
        "* Doing better with better prompts \n",
        "* Taking control of the generations\n",
        "  * Editing images\n",
        "  * Adding multiple conditions for generation\n",
        "  * Controlling semantic properties\n",
        "* Going beyond images -- text-to-video generation\n",
        "* Conclusion\n",
        "\n",
        "We believe these use cases will help you explore different creative applications and extend them further. \n",
        "\n",
        "Let's start diffusing ðŸ§¨"
      ],
      "metadata": {
        "id": "KLZWC4IQCDQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation and setup\n",
        "\n",
        "Since Colab comes with many pre-installed libraries already, we just need a few additional libraries to start our exploration:\n",
        "\n",
        "* [ðŸ§¨ diffusers](https://huggingface.co/docs/diffusers)\n",
        "* [ðŸ¤— accelerate](https://huggingface.co/docs/accelerate/)\n",
        "* [ðŸ¤— transformers](https://huggingface.co/docs/transformers/) "
      ],
      "metadata": {
        "id": "lATHJ8rHC5ry"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHA_E50hBj22"
      },
      "outputs": [],
      "source": [
        "!pip install -q diffusers accelerate transformers "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See if we have got access to a GPU (very important!)."
      ],
      "metadata": {
        "id": "htIY26PHEcpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "tsn0VIDuEcG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log in to your Hugging Face account as we will need to be authorized to access one of the models we'll use today:\n",
        "\n",
        "(If you don't have one, please [create one](https://huggingface.co/join); it's free ðŸ¤—)"
      ],
      "metadata": {
        "id": "GDXuXDGmE0TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "-a5eiXO5FNKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it for the setup part! We're now ready to start generating images ðŸŽ‡"
      ],
      "metadata": {
        "id": "5vNr66bLFWZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text-to-image generation in five LoC ðŸ–¼\n",
        "\n",
        "We start by importing the [`DiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline), which is a class that encapsulates all the logic involved behind a text-conditioned diffusion system for image generation. We initialize the pipeline from a pre-trained checkpoint: [`runwayml/stable-diffusion-v1-5`](https://hf.co/runwayml/stable-diffusion-v1-5).\n",
        "\n",
        "After importing the pipeline and initializing it, we call it on an input \"prompt\" which is basically an informal description in natural language of how we want the generated image to look like. We can also use an [unconditional diffusion model](https://huggingface.co/docs/diffusers/training/unconditional_training) that doesn't need a prompt but takes away user control. \n",
        "\n",
        "Generating images from a prompt is as easy as:"
      ],
      "metadata": {
        "id": "GzO_24o2FhzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(\"cuda\")\n",
        "prompt = \"sailing ship in storm by Leonardo da Vinci\"\n",
        "images = pipeline(prompt=prompt).images\n",
        "\n",
        "images[0]"
      ],
      "metadata": {
        "id": "xer5T7YTFeBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there we go! Within just five lines of code (LoC), we have generated an image from natural language input! \n",
        "\n",
        "> ðŸ’¡ **Note**: By default, `DiffusionPipeline` classes have a `safety_checker` enabled to prevent the use of NSFW content.\n",
        "\n",
        "As a good first next step, we can try changing the \"seed\" to see how that impacts the generated image. "
      ],
      "metadata": {
        "id": "TJnmB2gKGatG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "seed = 1554 #@param {type:\"integer\"}\n",
        "images = pipeline(prompt=prompt, generator=torch.manual_seed(seed)).images\n",
        "\n",
        "images[0]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MrhA7v-hHKAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to experiment with the seeds and notice how that impacts the quality of the generated image. \n",
        "\n",
        "We can also generate multiple images from the same prompt by specifying the `num_images_per_prompt` parameter:"
      ],
      "metadata": {
        "id": "x-KOQ9XuHpk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "\n",
        "def make_grid(images, rows, cols, resize_to=128):\n",
        "    images = [image.resize((resize_to, resize_to)) for image in images]\n",
        "    w, h = images[0].size\n",
        "    grid = PIL.Image.new(\"RGB\", size=(cols*w, rows*h))\n",
        "    for i, image in enumerate(images):\n",
        "        grid.paste(image, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "num_images_per_prompt = 4 #@param {type:\"integer\"}\n",
        "images = pipeline(prompt=prompt, num_images_per_prompt=num_images_per_prompt).images\n",
        "make_grid(images, 1, 4)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4MrTaV54JBK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have been performing all our computations in FP32 (floating-point 32 bits). Diffusers [supports](https://huggingface.co/docs/diffusers/main/en/optimization/fp16) FP16 computations which can speed up the generation speed without any degradation in the quality. It can also save you memory preventing OOM problems. FP16 computations are specifically advantageous on [GPUs having tensor cores](https://www.nvidia.com/en-in/data-center/tensor-cores/).  \n",
        "\n",
        "Let's how this is beneficial. \n",
        "\n",
        "We start by writing a utility function to benchmark the inference speed of our pipeline. "
      ],
      "metadata": {
        "id": "csu4BkW6K1xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "\n",
        "def benchmark(pipeline, prompt=\"a dog\"):\n",
        "    # warmup.\n",
        "    _ = pipeline(prompt=prompt) \n",
        "\n",
        "    # run for five iterations.\n",
        "    tic = time.time_ns()\n",
        "    for _ in range(5):\n",
        "        _ = pipeline(prompt=prompt)\n",
        "    tok = time.time_ns() \n",
        "    print(f\"Execution time -- {(tok - tic) / 1e6:.1f} ms\\n\")"
      ],
      "metadata": {
        "id": "1HUAdfdSMHrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We delete the previous `pipeline` and initialize two new pipelines:\n",
        "\n",
        "1. FP32 mode\n",
        "2. FP16 mode "
      ],
      "metadata": {
        "id": "TNKAa2uENjEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del pipeline \n",
        "fp32_pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(\"cuda\")\n",
        "fp16_pipeline = DiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16 # switching to FP16 mode is just this line of code :)\n",
        ").to(\"cuda\")"
      ],
      "metadata": {
        "id": "2Tg0gIAhM84C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take them on benchmarking â°"
      ],
      "metadata": {
        "id": "xCe_Y2P9Nob-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"*** FP32 Pipeline ***\")\n",
        "benchmark(fp32_pipeline)\n",
        "\n",
        "print(\"\\n*** FP16 Pipeline ***\")\n",
        "benchmark(fp16_pipeline)"
      ],
      "metadata": {
        "id": "ejoxYE9aNruS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The speedup is clear âš¡ï¸\n",
        "\n",
        "The speedup is more prominent when there are more images to generate. The numbers will also vary depending on the GPU we're using. \n",
        "\n",
        "Going forward in this notebook, we'll default to FP16 computations to save us some memory and speed. \n",
        "\n",
        "> ðŸ’¡ **Note**: Diffusers supports many forms of optimizations that can help you run the pipelines as optimally as possible with little to no code changes. These optimizations include attention slicing, CPU offloading, use of flash attention, etc. We encourage you to explore these options [here](https://huggingface.co/docs/diffusers/main/en/optimization/opt_overview).  "
      ],
      "metadata": {
        "id": "g7qGLmzyNrS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have been able to generate images but their quality (which is a subjective measurement) hasn't been super exciting. Even though there are various things we can do to improve that one particularly easy option is -- ***better prompting***. \n",
        "\n",
        "Let's explore this in the next section. "
      ],
      "metadata": {
        "id": "tSWRLTJlPJsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doing better with better prompts âœï¸\n",
        "\n",
        "Let's start with the following prompt:\n",
        "\n",
        "> \"portrait photo of a old warrior chief\""
      ],
      "metadata": {
        "id": "RXcG3nGiPgH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"portrait photo of a old warrior chief\"\n",
        "images = fp16_pipeline(prompt=prompt, num_images_per_prompt=4).images\n",
        "make_grid(images, rows=2, cols=2, resize_to=256)"
      ],
      "metadata": {
        "id": "0eZycUHPPfnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad but let's explore how we can do better. \n",
        "\n",
        "The text prompt you use to generate an image is super important, so much so that it is called ***prompt engineering***. Some considerations to keep during prompt engineering are:\n",
        "\n",
        "* How is the image or similar images of the one I want to generate stored on the internet?\n",
        "* What additional detail can I give that steers the model towards the style I want?\n",
        "\n",
        "With this in mind, letâ€™s improve the prompt to include color and higher-quality details:"
      ],
      "metadata": {
        "id": "3nPZ8QUQQ76d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt += \", tribal panther make up, blue on red, side profile, looking away, serious eyes\"\n",
        "prompt += \" 50mm portrait photography, hard rim lighting photography--beta --ar 2:3  --beta --upbeta\""
      ],
      "metadata": {
        "id": "eHsq3wcbQsVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = fp16_pipeline(prompt=prompt, num_images_per_prompt=4).images\n",
        "make_grid(images, rows=2, cols=2, resize_to=256)"
      ],
      "metadata": {
        "id": "vw2sDFFZRPBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty impressive! \n",
        "\n",
        "We encourage you to explore how much you push the needle further with better prompts (and other simple techniques) by referring to [this tutorial](https://huggingface.co/docs/diffusers/main/en/stable_diffusion). \n",
        "\n",
        "Up until this point, we've been experimenting with the \"vanilla\" text-to-image generation where we \"conditioned\" the diffusion model on text prompts. Let's now see how we can better control the images these models generate. "
      ],
      "metadata": {
        "id": "u5Z8jnndRhOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking control of the generations ðŸŽ›\n",
        "\n",
        "In this section, we'll explore:\n",
        "\n",
        "* how to edit images from natural language inputs\n",
        "* how to use multiple conditionings for generating images\n",
        "* how to control the semantic properties of the generated images\n",
        "\n",
        "Let's get controlling!\n",
        "\n"
      ],
      "metadata": {
        "id": "RZYDWdvUSQ_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Editing images\n",
        "\n",
        "We'll use [InstructPix2Pix](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/pix2pix) to edit images using text prompts. Along with an \"edit instruction\" (consider it to be prompt), we'd need to also supply an image that we wish to edit. \n",
        "\n",
        "We'll start by initializing the pipeline and loading the image we want to edit. "
      ],
      "metadata": {
        "id": "5DFqEu1rHV_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "model_id = \"timbrooks/instruct-pix2pix\"\n",
        "pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
        "    model_id, torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "original_image = load_image(\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Tower_Bridge_from_Shad_Thames.jpg/1200px-Tower_Bridge_from_Shad_Thames.jpg\")\n",
        "original_image"
      ],
      "metadata": {
        "id": "Cf4lGar6HpiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edit_instruction = \"Make it a picasso painting\"\n",
        "edited_image = pipeline(edit_instruction, image=original_image).images[0]\n",
        "edited_image"
      ],
      "metadata": {
        "id": "k2DfDOXdm72G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad. \n",
        "\n",
        "This pipeline exposes two important arguments:\n",
        "\n",
        "1. `image_guidance_scale` that lets us preserve the input image structure.\n",
        "2. `guidance_scale` that lets us control how much the edit instruction reflects in the edited image.\n",
        "\n",
        "Let's run the same pipeline across a few different `image_guidance_scale` and `guidance_scale` values. "
      ],
      "metadata": {
        "id": "rpWFxh63m7Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_cfg_scales = [1., 1.5, 2.]\n",
        "text_cfg_scales = [7., 10., 12.]\n",
        "edited_images = []\n",
        "\n",
        "for img_cfg in img_cfg_scales:\n",
        "    for text_cfg in text_cfg_scales:\n",
        "        image = pipeline(\n",
        "            edit_instruction,\n",
        "            image=original_image,\n",
        "            image_guidance_scale=img_cfg,\n",
        "            guidance_scale=text_cfg,\n",
        "            num_inference_steps=20,\n",
        "        ).images[0]\n",
        "        edited_images.append(image)"
      ],
      "metadata": {
        "id": "lk4zcATUomVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now plot these edited images to see the impact of different `image_guidance_scale` (denoted as $s_i$) and `guidance_scale` (denoted as $s_t$) values. "
      ],
      "metadata": {
        "id": "m3IYZE3iqYIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, image in enumerate(edited_images):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(np.array(image))\n",
        "    plt.title(\n",
        "        f\"$s_i$: {img_cfg_scales[i // 3]}, $s_t$: {text_cfg_scales[i % 3]}\",\n",
        "        fontsize=14,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "rKZtEfDeqWw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like the `image_guidance_scale` of 1.5 is essential for us to be able to properly reflect the edit instruction. \n",
        "\n",
        "Editing images seems practical. But what if we wanted to generate a new image from a geometric layout (a [canny edge map](https://en.wikipedia.org/wiki/Canny_edge_detector), for example) and a text prompt? Let's explore that now. "
      ],
      "metadata": {
        "id": "6PqX26YlvJSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding multiple conditions for generation\n",
        "\n",
        "For this part, we'll leverage [ControlNets](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet). We'll start by loading an image from which we'll extract its canny edge map. "
      ],
      "metadata": {
        "id": "HANcxFMQxD-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers.utils import load_image\n",
        "\n",
        "# Let's load the popular vermeer image\n",
        "image = load_image(\n",
        "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
        ")\n",
        "image"
      ],
      "metadata": {
        "id": "wvhcqlJBv9ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the canny edge map from the image: "
      ],
      "metadata": {
        "id": "qfbs6LQlyEHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "image = np.array(image)\n",
        "\n",
        "low_threshold = 100\n",
        "high_threshold = 200\n",
        "\n",
        "image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "image = image[:, :, None]\n",
        "image = np.concatenate([image, image, image], axis=2)\n",
        "canny_image = PIL.Image.fromarray(image)\n",
        "canny_image"
      ],
      "metadata": {
        "id": "EqILljx2x_ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can use this edge map as an additional conditioning with a text prompt to generate an image. But this time, let's be a little more creative. \n",
        "\n",
        "Instead of using the pre-trained [`runwayml/stable-diffusion-v1-5`](https://hf.co/runwayml/stable-diffusion-v1-5) checkpoint, let's use [one](https://huggingface.co/sd-dreambooth-library/mr-potato-head) that was trained to generate photos of [Mr. Potato Head](https://en.wikipedia.org/wiki/Mr._Potato_Head). \n",
        "\n",
        "In the next code cell, we initialize the [ControlNet model](https://huggingface.co/lllyasviel/sd-controlnet-canny) for handling canny edge maps as well as our [`StableDiffusionControlNetPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet#diffusers.StableDiffusionControlNetPipeline) (with the [`sd-dreambooth-library/mr-potato-head`](https://huggingface.co/sd-dreambooth-library/mr-potato-head) checkpoint)."
      ],
      "metadata": {
        "id": "9u4gv3mnyTWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"sd-dreambooth-library/mr-potato-head\", controlnet=controlnet, torch_dtype=torch.float16\n",
        ").to(\"cuda\")"
      ],
      "metadata": {
        "id": "HkPJ5tkNz7V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then generate: "
      ],
      "metadata": {
        "id": "9xczhYrV9LVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a photo of sks mr potato head, best quality, extremely detailed\"\n",
        "output = pipeline(prompt, canny_image)\n",
        "output.images[0]"
      ],
      "metadata": {
        "id": "lCxmHt-E0Oe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And voila! We have generated a Mr. Potato Head posing like the [famous Vermeer painting](https://en.wikipedia.org/wiki/Girl_with_a_Pearl_Earring) ðŸ˜‚\n",
        "\n",
        "It's possible to combine multiple conditionings (e.g., combining canny edge map and pose) for generation. Refer to the [docs](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet#combining-multiple-conditionings) to know more! \n",
        "\n",
        "Text-to-image generation is fun! It can help unlock different creative avenues that may have seemed impossible to explore before. But what if we wanted to have more control over the semantic properties of the generated images, such as -- smile factor of an image involving a human face?"
      ],
      "metadata": {
        "id": "JcdG6l-E0pKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Controlling semantic properties\n",
        "\n",
        "We will control the semantic properties of the generated images through something called \"semantic guidance\" via the [`SemanticStableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/semantic_stable_diffusion). To understand this better, let's first generate an image with the following prompt -- \"a photo of the face of a woman\".\n",
        "\n"
      ],
      "metadata": {
        "id": "S3GQP6vk5J8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a photo of the face of a woman\"\n",
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# We're fixing the seed to have a more deterministic way of\n",
        "# comparing the outputs here. Any diffusion pipeline in Diffusers\n",
        "# comes with a `generator` argument in its __call__ method to\n",
        "# help fix the seed. \n",
        "pipeline(prompt=prompt, generator=torch.manual_seed(0)).images[0]"
      ],
      "metadata": {
        "id": "Hi7rY71-52Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now leverage the `SemanticStableDiffusionPipeline` to semantically control the output image by specifying what attributes we want to control. "
      ],
      "metadata": {
        "id": "alqwmdJdfmcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import SemanticStableDiffusionPipeline\n",
        "\n",
        "pipeline = SemanticStableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
        ")\n",
        "pipeline = pipeline.to(\"cuda\")\n",
        "\n",
        "out = pipeline(\n",
        "    prompt=\"a photo of the face of a woman\",\n",
        "    num_images_per_prompt=1,\n",
        "    guidance_scale=7,\n",
        "    editing_prompt=[\n",
        "        # Concepts to apply\n",
        "        \"smiling, smile\",  \n",
        "        \"glasses, wearing glasses\",\n",
        "    ],\n",
        "    reverse_editing_direction=[True, False],  # Direction of guidance i.e. increase all concepts\n",
        "    edit_warmup_steps=[10, 10],  # Warmup period for each concept\n",
        "    edit_guidance_scale=[4, 5],  # Guidance scale for each concept\n",
        "    edit_threshold=[\n",
        "        0.99,\n",
        "        0.975,\n",
        "    ],  # Threshold for each concept. Threshold equals the percentile of the latent space that will be discarded. I.e. threshold=0.99 uses 1% of the latent dimensions\n",
        "    edit_momentum_scale=0.3,  # Momentum scale that will be added to the latent guidance\n",
        "    edit_mom_beta=0.6,  # Momentum beta\n",
        "    edit_weights=[1, 1],  # Weights of the individual concepts against each other\n",
        "    generator=torch.manual_seed(0)\n",
        ")\n",
        "out.images[0]"
      ],
      "metadata": {
        "id": "NEGIhTDf6oGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty amazing! Now, we can control the semantic properties of the generated images as we did here: \n",
        "\n",
        "* Decrease the smile quotient a bit\n",
        "* Make the subject wear glasses\n",
        "\n",
        "It's okay if we don't understand all the arguments used in the above pipeline. The most important ones are:\n",
        "\n",
        "* `editing_prompt`\n",
        "* `edit_guidance_scale`\n",
        "* `edit_weights`\n",
        "\n",
        "If this type of controllable generation fascinates you, we welcome you to check the [official documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/semantic_stable_diffusion) and the [SEGA paper](https://arxiv.org/abs/2301.12247). \n",
        "\n",
        "With this, we conclude our exploration of controllable image generation. To learn more about other similar techniques, check out [our documentation](https://huggingface.co/docs/diffusers/main/en/using-diffusers/controlling_generation). \n",
        "\n",
        "In the final leg, we'll take Diffusers to go beyond images and generate videos from text prompts ðŸ¤¯"
      ],
      "metadata": {
        "id": "bg02vuxv7wKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Going beyond images -- text-to-video generation ðŸŽ¥\n",
        "\n",
        "It turns out that it's possible to extend a text-to-image diffusion system such that it generates temporally coherent video content. To the best of our knowledge, it was first explored in [Text-to-Video Zero](https://huggingface.co/docs/diffusers/main/en/api/pipelines/text_to_video_zero). \n",
        "\n",
        "With the [`TextToVideoZeroPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/text_to_video_zero#diffusers.TextToVideoZeroPipeline), the interface for generating a video from a prompt remains almost the same as our text-to-image generation pipeline ðŸ¤—\n",
        "\n",
        "Let's first clear some memory as video generation is a memory-heavy task.\n",
        "\n"
      ],
      "metadata": {
        "id": "z5MbjwlL85jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the unused heavy objects.\n",
        "del pipeline, controlnet\n",
        "\n",
        "# Let's free some memory. \n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wOR1-sBmzpc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, let's generate a video!"
      ],
      "metadata": {
        "id": "Qh_IDyHMgJYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "from diffusers import TextToVideoZeroPipeline\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = TextToVideoZeroPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "prompt = \"A panda playing guitar in the mountains\"\n",
        "result = pipe(prompt=prompt).images\n",
        "result = [(r * 255).astype(\"uint8\") for r in result]\n",
        "imageio.mimsave(\"video.mp4\", result, fps=2)"
      ],
      "metadata": {
        "id": "JN8HdPLKuV8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### Show video\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(\"video.mp4\", \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S2vCwOTOu-JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's cute! \n",
        "\n",
        "Just like we controlled different aspects of the generated images, we can do the same for videos too! For example, we can condition the video generation process on certain poses along with the prompt. \n",
        "\n",
        "And the good news is that we can leverage the components we have already seen for doing this:\n",
        "\n",
        "* `ControlNet`\n",
        "* `StableDiffusionControlNetPipeline`\n",
        "\n",
        "To make the generated video frames temporally coherent, we'll leverage the special [`CrossFrameAttnProcessor`](https://github.com/huggingface/diffusers/blob/7a32b6beeb0cfdefed645253dce23d9b0a78597f/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py#L39) class. Diffusers supports many [\"attention processor\" classes](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py) for different things. A discussion on that is out of the scope of this session. \n",
        "\n",
        "We first download a video whose frames are comprised of different poses but temporally coherent:"
      ],
      "metadata": {
        "id": "Hlym27nAv2mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "filename = \"__assets__/poses_skeleton_gifs/dance1_corr.mp4\"\n",
        "repo_id = \"PAIR/Text2Video-Zero\"\n",
        "video_path = hf_hub_download(repo_type=\"space\", repo_id=repo_id, filename=filename)"
      ],
      "metadata": {
        "id": "tsv82jt72KJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### Show video\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(video_path, \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r0xhcuWi3A83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we extract the individual frames from the video sequentially: "
      ],
      "metadata": {
        "id": "WfuwX77R2ToZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import imageio\n",
        "\n",
        "reader = imageio.get_reader(video_path, \"ffmpeg\")\n",
        "frame_count = 8\n",
        "pose_images = [Image.fromarray(reader.get_data(i)) for i in range(frame_count)]"
      ],
      "metadata": {
        "id": "0snRNJXW2Lo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we initialize the `ControlNet` model for poses and the `StableDiffusionControlNetPipeline`. We then assign `CrossFrameAttnProcessor` to be the attention processor of `StableDiffusionControlNetPipeline`. "
      ],
      "metadata": {
        "id": "98FQ8dO52npe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers.pipelines.text_to_video_synthesis.pipeline_text_to_video_zero import CrossFrameAttnProcessor\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-openpose\", torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Set the attention processor.\n",
        "pipeline.unet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))\n",
        "pipeline.controlnet.set_attn_processor(CrossFrameAttnProcessor(batch_size=2))"
      ],
      "metadata": {
        "id": "pakBhlOd2Yv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can make Darth Vader dance in the poses we want:"
      ],
      "metadata": {
        "id": "chrp3tao3KtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix latents for all frames. This is to avoid unexpected randomness in the process.\n",
        "latents = torch.randn((1, 4, 64, 64), device=\"cuda\", dtype=torch.float16).repeat(len(pose_images), 1, 1, 1)\n",
        "\n",
        "prompt = \"Darth Vader dancing in a desert\"\n",
        "result = pipeline(prompt=[prompt] * len(pose_images), image=pose_images, latents=latents).images\n",
        "imageio.mimsave(\"video.mp4\", result, fps=4)"
      ],
      "metadata": {
        "id": "u9k2akgX3PMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##### Show video\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(\"video.mp4\", \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "c00WDNFo33a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it for all the code cavalry for today's session. Let's quickly recap what we learned and how we can explore Diffusers further to unlock the massive creative potential lying ahead."
      ],
      "metadata": {
        "id": "mKC3bAOK5GMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "We covered quite of a lot things today starting from text-to-image generation all the way up to text-to-video generation. In between, we learned how to improve the quality of the generated images with better prompts. We also learned to control different aspects of the generated images by exploring different techniques. \n",
        "\n",
        "With that said, we barely scratched the surface of Diffusers today. There's a lot more we can do with it. Below are some interesting pointers for you to explore further if this area of work seems interesting to you:\n",
        "\n",
        "* [Running](https://huggingface.co/blog/if) heavy pipelines on consumer GPUs\n",
        "* [Training](https://huggingface.co/docs/diffusers/main/en/training/overview) your own diffusion models easily \n",
        "* [Models](https://huggingface.co/docs/diffusers/main/en/api/pipelines/if) that are particularly good at generating text content in images\n",
        "* [Audio generation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audio_diffusion) with Diffusers\n",
        "\n",
        "Ciao!\n",
        "\n",
        "<div align=\"center\">\n",
        "<figure>\n",
        "<img src=\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/datacamp_bunny.png\" width=500/>\n",
        "<figcaption>Image generated with <a href=\"https://huggingface.co/spaces/DeepFloyd/IF\">IF</a>. Prompt: high quality dslr photo of a happy bunny holding a sign saying \"DataCamp\"</figcaption>\n",
        "</figure>\n",
        "</div>"
      ],
      "metadata": {
        "id": "EMh0kf0E5YZB"
      }
    }
  ]
}